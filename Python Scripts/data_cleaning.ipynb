{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from permetrics.regression import Metrics\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.linear_model import Ridge, LinearRegression, Lasso, ElasticNet\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from dython import nominal\n",
    "import graphviz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lead behandling and select relevant columns\n",
    "behandling = pd.read_csv(Path('../20210324/with_name/behandling_optillfälle.csv'), sep=';')\n",
    "behandling = behandling[['Der_Behandling_PK',\n",
    "                         'Der_Opkort_FK',\n",
    "                         'Der_Anestesikort_FK',\n",
    "                         'Der_Vårdform_FK',\n",
    "                         'Der_Prioritet_FK',\n",
    "                         'BehandlingsStatus',\n",
    "                         'ASAklass',\n",
    "                         'ForberedelsetidStartTidpunkt',\n",
    "                         'ForberedelsetidSlutTidpunkt',\n",
    "                         'PatientÅlderVidOp',\n",
    "                         'Veckodag',\n",
    "                         'Starttimme',\n",
    "                         'BMI',\n",
    "                         'Kroppslängd',\n",
    "                         'Kroppsvikt',\n",
    "                         'OperationstidStart',\n",
    "                         'AnestesitidStart'\n",
    "                        ]]\n",
    "behandling = behandling[behandling['BehandlingsStatus'] == 'Opererad'] # Remove 'abrutna' operationer as they do not contain all relevant data\n",
    "print(\"Behandling length: {}\".format(len(behandling)))\n",
    "\n",
    "# Load ingrepp and select relevant columns\n",
    "ingrepp = pd.read_csv(Path('../20210324/with_name/op_ingrepp_namn.csv'))\n",
    "ingrepp = ingrepp[['Der_Behandling_PK',\n",
    "                   'Ingreppkod',\n",
    "                   'Primär_Sekundär',\n",
    "                   'Sida',\n",
    "                  ]]\n",
    "ingrepp = ingrepp[ingrepp['Primär_Sekundär'] == 'Primär'] # Might want to include this if we make a more complicated model\n",
    "print(\"Ingrepp length: {}\".format(len(ingrepp)))\n",
    "\n",
    "# Load diagnos and select relevant columns\n",
    "diagnos = pd.read_csv(Path('../20210324/with_name/op_diagnos_namn.csv'))\n",
    "diagnos = diagnos[['Der_Behandling_PK',\n",
    "                   'Diagnoskod',\n",
    "                   'Primär_Sekundär',\n",
    "                  ]]\n",
    "diagnos = diagnos[diagnos['Primär_Sekundär'] == 'Primär'] # Might want to include this if we make a more complicated model\n",
    "print(\"Diagnos length: {}\".format(len(diagnos)))\n",
    "\n",
    "# Combine the data frames\n",
    "combined_df = behandling.merge(diagnos, on='Der_Behandling_PK').merge(ingrepp, on='Der_Behandling_PK')\n",
    "combined_df = combined_df.dropna()\n",
    "print(\"Combined length: {}\".format(len(combined_df)))\n",
    "\n",
    "ingreppsgrupp = []\n",
    "for index, row in combined_df.iterrows():\n",
    "    ingrepp = row['Ingreppkod']\n",
    "    ingrepp_group = ingrepp[0:2]\n",
    "    ingreppsgrupp.append(ingrepp_group)\n",
    "combined_df['IngreppsGrupp'] = ingreppsgrupp\n",
    "\n",
    "diagnosgrupp = []\n",
    "for index, row in combined_df.iterrows():\n",
    "    diagnos = row['Diagnoskod']\n",
    "    diagnos_grupp = diagnos[0]\n",
    "    diagnosgrupp.append(diagnos_grupp)\n",
    "combined_df['DiagnosGrupp'] = diagnosgrupp\n",
    "\n",
    "# Calculate and add time to the dataframe\n",
    "# Bad algoritm for checking min and max time of förbereds\n",
    "start_pre = combined_df[\"ForberedelsetidStartTidpunkt\"]\n",
    "slut_pre = combined_df[\"ForberedelsetidSlutTidpunkt\"]\n",
    "start_an = combined_df['AnestesitidStart']\n",
    "start_op = combined_df['OperationstidStart']\n",
    "\n",
    "start_times = []\n",
    "for time in start_pre:\n",
    "    minn = int(time[-9:-7])\n",
    "    hour = int(time[-12:-10])\n",
    "    minutes = hour*60 + minn\n",
    "    start_times.append(minutes)\n",
    "    \n",
    "stop_times = []\n",
    "for time in slut_pre:\n",
    "    minn = int(time[-9:-7])\n",
    "    hour = int(time[-12:-10])\n",
    "    minutes = hour*60 + minn\n",
    "    stop_times.append(minutes)\n",
    "\n",
    "an_times = []\n",
    "for time in start_an:\n",
    "    minn = int(time[-9:-7])\n",
    "    hour = int(time[-12:-10])\n",
    "    minutes = hour*60 + minn\n",
    "    an_times.append(minutes)\n",
    "    \n",
    "op_times = []\n",
    "for time in start_op:\n",
    "    minn = int(time[-9:-7])\n",
    "    hour = int(time[-12:-10])\n",
    "    minutes = hour*60 + minn\n",
    "    op_times.append(minutes)\n",
    "\n",
    "times = []\n",
    "for i in range(len(start_times)):\n",
    "    #print(stop_times[i], start_times[i], stop_times[i] - start_times[i])\n",
    "    times.append((stop_times[i] - start_times[i]) + (an_times[i] - op_times[i])) # Förberedelsetid + anestesiförberedelsetid\n",
    "    \n",
    "# Add total time to dataframe\n",
    "combined_df['Tid'] = times\n",
    "\n",
    "# Remove all fetuers we don't want\n",
    "features_df = combined_df.drop([\"Der_Behandling_PK\", \n",
    "                               \"Der_Opkort_FK\",\n",
    "                               \"Der_Anestesikort_FK\",\n",
    "                               \"BehandlingsStatus\",\n",
    "                               \"ForberedelsetidStartTidpunkt\",\n",
    "                               \"ForberedelsetidSlutTidpunkt\",\n",
    "                               \"Primär_Sekundär_x\",\n",
    "                               \"Primär_Sekundär_y\",\n",
    "                                \"AnestesitidStart\",\n",
    "                                \"OperationstidStart\"\n",
    "                            ], axis='columns')\n",
    "\n",
    "\n",
    "'''\n",
    "diagnosgrupper = {}\n",
    "for diagnosgrupp, diagnosgrupp_df in features_df.groupby('DiagnosGrupp'):\n",
    "    diagnosgrupper[diagnosgrupp] = diagnosgrupp_df\n",
    "grupp_mean = []\n",
    "grupp_std = []\n",
    "for grupp in diagnosgrupper.keys():\n",
    "    df = features_df[features_df['DiagnosGrupp'] == grupp]\n",
    "    grupp_mean.append(df['time'].mean())\n",
    "    grupp_std.append(df['time'].std())\n",
    "#plt.errorbar(diagnosgrupper.keys(), grupp_mean, grupp_std, marker='o', linestyle='None', capsize=3)\n",
    "\n",
    "ingreppsgrupper = {}\n",
    "for ingreppsgrupp, ingreppsgrupp_df in features_df.groupby('IngreppsGrupp'):\n",
    "    ingreppsgrupper[ingreppsgrupp] = ingreppsgrupp_df\n",
    "grupp_mean = []\n",
    "grupp_std = []\n",
    "for grupp in ingreppsgrupper.keys():\n",
    "    df = features_df[features_df['IngreppsGrupp'] == grupp]\n",
    "    grupp_mean.append(df['time'].mean())\n",
    "    grupp_std.append(df['time'].std())\n",
    "#plt.errorbar(ingreppsgrupper.keys(), grupp_mean, grupp_std, marker='o', linestyle='None', capsize=3)\n",
    "'''\n",
    "features_df = features_df.drop([\"Diagnoskod\", \"Ingreppkod\"], axis='columns')\n",
    "features_df = features_df[features_df['IngreppsGrupp'].isin(['NC', 'NH', 'NB', 'NG', 'NF', 'ND'])]\n",
    "\n",
    "# Instansiate Metrics so we can use MAAPE later\n",
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle NaN (ONLY DO ONE OF THESE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove rows with NaN** (Good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = features_df.dropna()\n",
    "y = features_df[\"Tid\"]\n",
    "X = features_df.drop(\"Tid\", axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding (ONLY DO ONE OF THESE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use One Hot Encoding to encode \"sida\" and \"ingreppsgrupp\"** (This seems to be the better alternative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(X)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state=66)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.2, random_state=66)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try dummyregressor as a basecase, anything worse than this is really terrible\n",
    "regr = DummyRegressor()\n",
    "regr.fit(X_train, y_train)\n",
    "pred = regr.predict(X_test)\n",
    "abs_error = mean_squared_error(y_test, pred, squared=False)\n",
    "percentage_error = metrics.mean_arctangent_absolute_percentage_error(clean=True, y_pred=np.array(pred), y_true=np.array(y_test))\n",
    "r2 = r2_score(y_test, pred)\n",
    "print(f'abs error:{abs_error} \\n% error: {percentage_error} \\nR-squared: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = DecisionTreeRegressor()\n",
    "regr.fit(X_train, y_train)\n",
    "pred = regr.predict(X_test)\n",
    "abs_error = mean_squared_error(y_test, pred, squared=False)\n",
    "percentage_error = metrics.mean_arctangent_absolute_percentage_error(clean=True, y_pred=np.array(pred), y_true=np.array(y_test))\n",
    "r2 = r2_score(y_test, pred)\n",
    "print(f'abs error:{abs_error} \\n% error: {percentage_error} \\nR-squared: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_regr = RandomForestRegressor(max_depth=22)\n",
    "forest_regr.fit(X_train, y_train)\n",
    "pred = forest_regr.predict(X_test)\n",
    "abs_error = mean_squared_error(y_test, pred, squared=False)\n",
    "percentage_error = metrics.mean_arctangent_absolute_percentage_error(clean=True, y_pred=np.array(pred), y_true=np.array(y_test))\n",
    "r2 = r2_score(y_test, pred)\n",
    "print(f'abs error:{abs_error} \\n% error: {percentage_error} \\nR-squared: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost_regr = GradientBoostingRegressor(max_depth=3,)\n",
    "boost_regr.fit(X_train, y_train)\n",
    "pred = boost_regr.predict(X_test)\n",
    "abs_error = mean_squared_error(y_test, pred, squared=False)\n",
    "percentage_error = metrics.mean_arctangent_absolute_percentage_error(clean=True, y_pred=np.array(pred), y_true=np.array(y_test))\n",
    "r2 = r2_score(y_test, pred)\n",
    "print(f'abs error:{abs_error} \\n% error: {percentage_error} \\nR-squared: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = MLPRegressor(random_state=1, activation='logistic', learning_rate='adaptive', )\n",
    "regr.fit(X_train, y_train)\n",
    "pred = regr.predict(X_test)\n",
    "abs_error = mean_squared_error(y_test, pred, squared=False)\n",
    "percentage_error = metrics.mean_arctangent_absolute_percentage_error(clean=True, y_pred=np.array(pred), y_true=np.array(y_test))\n",
    "r2 = r2_score(y_test, pred)\n",
    "print(f'abs error:{abs_error} \\n% error: {percentage_error} \\nR-squared: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "pred = regr.predict(X_test)\n",
    "abs_error = mean_squared_error(y_test, pred, squared=False)\n",
    "percentage_error = metrics.mean_arctangent_absolute_percentage_error(clean=True, y_pred=np.array(pred), y_true=np.array(y_test))\n",
    "r2 = r2_score(y_test, pred)\n",
    "print(f'abs error:{abs_error} \\n% error: {percentage_error} \\nR-squared: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(31, input_dim=31, kernel_initializer='normal', activation='relu')) #Input layer\n",
    "    model.add(Dense(10, kernel_initializer='normal', activation='relu')) #Input layer\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='relu')) #Output layer\n",
    "    # Compile model\n",
    "    opt = keras.optimizers.Adam(clipnorm=1, learning_rate=0.001)\n",
    "    model.compile(loss='mean_squared_error', optimizer=opt)\n",
    "    return model\n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=1000, batch_size=32, verbose=1)\n",
    "kfold = KFold(n_splits=10)\n",
    "#results = cross_val_score(estimator, X_train, y_train, cv=kfold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avrg = sum(results)/len(results)\n",
    "print(f'avrg mean_error_sqr: {avrg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator.fit(X_train, y_train)\n",
    "pred = estimator.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get importance of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = permutation_importance(regr, X.toarray(), y, n_repeats=10, random_state=0)\n",
    "print(result.importances_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = features_df.rename(columns={'Der_Vårdform_FK':'Vårdform', 'Der_Prioritet_FK':'Prioritet', 'Tid':'Förberedeletid'})\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "nominal.associations(features_df, nominal_columns=['IngreppsGrupp', 'Sida', 'DiagnosGrupp'], ax=ax, theil_u=True)\n",
    "fig.savefig('corr-matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_regression(X, y,\n",
    "                       threshold_in,\n",
    "                       verbose=True):\n",
    "    initial_list = []\n",
    "    included = list(initial_list)\n",
    "    while True:\n",
    "        changed=False\n",
    "        excluded = list(set(X.columns)-set(included))\n",
    "        new_pval = pd.Series(index=excluded)\n",
    "        for new_column in excluded:\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]].astype('float64')))).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        best_pval = new_pval.min()\n",
    "        if best_pval < threshold_in:\n",
    "            best_feature = new_pval.idxmin()\n",
    "            included.append(best_feature)\n",
    "            changed=True\n",
    "            if verbose:\n",
    "                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n",
    "\n",
    "        if not changed:\n",
    "            break\n",
    "\n",
    "    return included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_regression(X, y, 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "features_df = features_df.dropna()\n",
    "y = features_df[\"Tid\"]\n",
    "X = features_df.drop(\"Tid\", axis='columns')### Try with only p<0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try without Vårdform, prioritet, asaklass, patientålder, kroppsvikt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lead behandling and select relevant columns\n",
    "behandling = pd.read_csv(Path('../20210324/with_name/behandling_optillfälle.csv'), sep=';')\n",
    "behandling = behandling[['Der_Behandling_PK',\n",
    "                         'Der_Opkort_FK',\n",
    "                         'Der_Anestesikort_FK',\n",
    "                         'Der_Vårdform_FK',\n",
    "                         'Der_Prioritet_FK',\n",
    "                         'BehandlingsStatus',\n",
    "                         'ASAklass',\n",
    "                         'ForberedelsetidStartTidpunkt',\n",
    "                         'ForberedelsetidSlutTidpunkt',\n",
    "                         'PatientÅlderVidOp',\n",
    "                         'Veckodag',\n",
    "                         'Starttimme',\n",
    "                         'BMI',\n",
    "                         'Kroppslängd',\n",
    "                         'Kroppsvikt',\n",
    "                         'OperationstidStart',\n",
    "                         'AnestesitidStart'\n",
    "                        ]]\n",
    "behandling = behandling[behandling['BehandlingsStatus'] == 'Opererad'] # Remove 'abrutna' operationer as they do not contain all relevant data\n",
    "print(\"Behandling length: {}\".format(len(behandling)))\n",
    "\n",
    "# Load ingrepp and select relevant columns\n",
    "ingrepp = pd.read_csv(Path('../20210324/with_name/op_ingrepp_namn.csv'))\n",
    "ingrepp = ingrepp[['Der_Behandling_PK',\n",
    "                   'Ingreppkod',\n",
    "                   'Primär_Sekundär',\n",
    "                   'Sida',\n",
    "                  ]]\n",
    "ingrepp = ingrepp[ingrepp['Primär_Sekundär'] == 'Primär'] # Might want to include this if we make a more complicated model\n",
    "print(\"Ingrepp length: {}\".format(len(ingrepp)))\n",
    "\n",
    "# Load diagnos and select relevant columns\n",
    "diagnos = pd.read_csv(Path('../20210324/with_name/op_diagnos_namn.csv'))\n",
    "diagnos = diagnos[['Der_Behandling_PK',\n",
    "                   'Diagnoskod',\n",
    "                   'Primär_Sekundär',\n",
    "                  ]]\n",
    "diagnos = diagnos[diagnos['Primär_Sekundär'] == 'Primär'] # Might want to include this if we make a more complicated model\n",
    "print(\"Diagnos length: {}\".format(len(diagnos)))\n",
    "\n",
    "# Combine the data frames\n",
    "combined_df = behandling.merge(diagnos, on='Der_Behandling_PK').merge(ingrepp, on='Der_Behandling_PK')\n",
    "combined_df = combined_df.dropna()\n",
    "print(\"Combined length: {}\".format(len(combined_df)))\n",
    "\n",
    "ingreppsgrupp = []\n",
    "for index, row in combined_df.iterrows():\n",
    "    ingrepp = row['Ingreppkod']\n",
    "    ingrepp_group = ingrepp[0:2]\n",
    "    ingreppsgrupp.append(ingrepp_group)\n",
    "combined_df['IngreppsGrupp'] = ingreppsgrupp\n",
    "\n",
    "diagnosgrupp = []\n",
    "for index, row in combined_df.iterrows():\n",
    "    diagnos = row['Diagnoskod']\n",
    "    diagnos_grupp = diagnos[0]\n",
    "    diagnosgrupp.append(diagnos_grupp)\n",
    "combined_df['DiagnosGrupp'] = diagnosgrupp\n",
    "\n",
    "# Calculate and add time to the dataframe\n",
    "# Bad algoritm for checking min and max time of förbereds\n",
    "start_pre = combined_df[\"ForberedelsetidStartTidpunkt\"]\n",
    "slut_pre = combined_df[\"ForberedelsetidSlutTidpunkt\"]\n",
    "start_an = combined_df['AnestesitidStart']\n",
    "start_op = combined_df['OperationstidStart']\n",
    "\n",
    "start_times = []\n",
    "for time in start_pre:\n",
    "    minn = int(time[-9:-7])\n",
    "    hour = int(time[-12:-10])\n",
    "    minutes = hour*60 + minn\n",
    "    start_times.append(minutes)\n",
    "    \n",
    "stop_times = []\n",
    "for time in slut_pre:\n",
    "    minn = int(time[-9:-7])\n",
    "    hour = int(time[-12:-10])\n",
    "    minutes = hour*60 + minn\n",
    "    stop_times.append(minutes)\n",
    "\n",
    "an_times = []\n",
    "for time in start_an:\n",
    "    minn = int(time[-9:-7])\n",
    "    hour = int(time[-12:-10])\n",
    "    minutes = hour*60 + minn\n",
    "    an_times.append(minutes)\n",
    "    \n",
    "op_times = []\n",
    "for time in start_op:\n",
    "    minn = int(time[-9:-7])\n",
    "    hour = int(time[-12:-10])\n",
    "    minutes = hour*60 + minn\n",
    "    op_times.append(minutes)\n",
    "\n",
    "times = []\n",
    "for i in range(len(start_times)):\n",
    "    #print(stop_times[i], start_times[i], stop_times[i] - start_times[i])\n",
    "    times.append((stop_times[i] - start_times[i]) + (an_times[i] - op_times[i])) # Förberedelsetid + anestesiförberedelsetid\n",
    "    \n",
    "# Add total time to dataframe\n",
    "combined_df['Tid'] = times\n",
    "\n",
    "# Remove all fetuers we don't want\n",
    "features_df = combined_df.drop([\"Der_Behandling_PK\", \n",
    "                               \"Der_Opkort_FK\",\n",
    "                               \"Der_Anestesikort_FK\",\n",
    "                               \"BehandlingsStatus\",\n",
    "                               \"ForberedelsetidStartTidpunkt\",\n",
    "                               \"ForberedelsetidSlutTidpunkt\",\n",
    "                               \"Primär_Sekundär_x\",\n",
    "                               \"Primär_Sekundär_y\",\n",
    "                                \"AnestesitidStart\",\n",
    "                                \"OperationstidStart\"\n",
    "                            ], axis='columns')\n",
    "\n",
    "\n",
    "'''\n",
    "diagnosgrupper = {}\n",
    "for diagnosgrupp, diagnosgrupp_df in features_df.groupby('DiagnosGrupp'):\n",
    "    diagnosgrupper[diagnosgrupp] = diagnosgrupp_df\n",
    "grupp_mean = []\n",
    "grupp_std = []\n",
    "for grupp in diagnosgrupper.keys():\n",
    "    df = features_df[features_df['DiagnosGrupp'] == grupp]\n",
    "    grupp_mean.append(df['time'].mean())\n",
    "    grupp_std.append(df['time'].std())\n",
    "#plt.errorbar(diagnosgrupper.keys(), grupp_mean, grupp_std, marker='o', linestyle='None', capsize=3)\n",
    "\n",
    "ingreppsgrupper = {}\n",
    "for ingreppsgrupp, ingreppsgrupp_df in features_df.groupby('IngreppsGrupp'):\n",
    "    ingreppsgrupper[ingreppsgrupp] = ingreppsgrupp_df\n",
    "grupp_mean = []\n",
    "grupp_std = []\n",
    "for grupp in ingreppsgrupper.keys():\n",
    "    df = features_df[features_df['IngreppsGrupp'] == grupp]\n",
    "    grupp_mean.append(df['time'].mean())\n",
    "    grupp_std.append(df['time'].std())\n",
    "#plt.errorbar(ingreppsgrupper.keys(), grupp_mean, grupp_std, marker='o', linestyle='None', capsize=3)\n",
    "'''\n",
    "features_df = features_df.drop([\"Diagnoskod\", \"Ingreppkod\"], axis='columns')\n",
    "features_df = features_df[features_df['IngreppsGrupp'].isin(['NC', 'NH', 'NB', 'NG', 'NF', 'ND'])]\n",
    "\n",
    "# Instansiate Metrics so we can use MAAPE later\n",
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = features_df.drop(['PatientÅlderVidOp', 'ASAklass', 'PatientÅlderVidOp'], axis=1)\n",
    "features_df = features_df.dropna()\n",
    "y = features_df[\"Tid\"]\n",
    "X = features_df.drop(\"Tid\", axis='columns')\n",
    "\n",
    "X = pd.get_dummies(X)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state=66)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.2, random_state=66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "nominal.associations(features_df, nominal_columns=['IngreppsGrupp', 'Sida', 'DiagnosGrupp'], ax=ax, theil_u=True)\n",
    "fig.savefig('corr-matri_reduced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try dummyregressor as a basecase, anything worse than this is really terrible\n",
    "regr = DummyRegressor()\n",
    "regr.fit(X_train, y_train)\n",
    "pred = regr.predict(X_test)\n",
    "abs_error = mean_squared_error(y_test, pred, squared=False)\n",
    "percentage_error = metrics.mean_arctangent_absolute_percentage_error(clean=True, y_pred=np.array(pred), y_true=np.array(y_test))\n",
    "r2 = r2_score(y_test, pred)\n",
    "print(f'abs error:{abs_error} \\n% error: {percentage_error} \\nR-squared: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = DecisionTreeRegressor()\n",
    "regr.fit(X_train, y_train)\n",
    "pred = regr.predict(X_test)\n",
    "abs_error = mean_squared_error(y_test, pred, squared=False)\n",
    "percentage_error = metrics.mean_arctangent_absolute_percentage_error(clean=True, y_pred=np.array(pred), y_true=np.array(y_test))\n",
    "r2 = r2_score(y_test, pred)\n",
    "print(f'abs error:{abs_error} \\n% error: {percentage_error} \\nR-squared: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_regr = RandomForestRegressor(max_depth=22)\n",
    "forest_regr.fit(X_train, y_train)\n",
    "pred = forest_regr.predict(X_test)\n",
    "abs_error = mean_squared_error(y_test, pred, squared=False)\n",
    "percentage_error = metrics.mean_arctangent_absolute_percentage_error(clean=True, y_pred=np.array(pred), y_true=np.array(y_test))\n",
    "r2 = r2_score(y_test, pred)\n",
    "print(f'abs error:{abs_error} \\n% error: {percentage_error} \\nR-squared: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost_regr = GradientBoostingRegressor(max_depth=3,)\n",
    "boost_regr.fit(X_train, y_train)\n",
    "pred = boost_regr.predict(X_test)\n",
    "abs_error = mean_squared_error(y_test, pred, squared=False)\n",
    "percentage_error = metrics.mean_arctangent_absolute_percentage_error(clean=True, y_pred=np.array(pred), y_true=np.array(y_test))\n",
    "r2 = r2_score(y_test, pred)\n",
    "print(f'abs error:{abs_error} \\n% error: {percentage_error} \\nR-squared: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = MLPRegressor(random_state=1, activation='logistic', learning_rate='adaptive', )\n",
    "regr.fit(X_train, y_train)\n",
    "pred = regr.predict(X_test)\n",
    "abs_error = mean_squared_error(y_test, pred, squared=False)\n",
    "percentage_error = metrics.mean_arctangent_absolute_percentage_error(clean=True, y_pred=np.array(pred), y_true=np.array(y_test))\n",
    "r2 = r2_score(y_test, pred)\n",
    "print(f'abs error:{abs_error} \\n% error: {percentage_error} \\nR-squared: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "pred = regr.predict(X_test)\n",
    "abs_error = mean_squared_error(y_test, pred, squared=False)\n",
    "percentage_error = metrics.mean_arctangent_absolute_percentage_error(clean=True, y_pred=np.array(pred), y_true=np.array(y_test))\n",
    "r2 = r2_score(y_test, pred)\n",
    "print(f'abs error:{abs_error} \\n% error: {percentage_error} \\nR-squared: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lead behandling and select relevant columns\n",
    "behandling = pd.read_csv(Path('../20210324/with_name/behandling_optillfälle.csv'), sep=';')\n",
    "behandling = behandling[['Der_Behandling_PK',\n",
    "                         'Der_Opkort_FK',\n",
    "                         'Der_Anestesikort_FK',\n",
    "                         'Der_Vårdform_FK',\n",
    "                         'Der_Prioritet_FK',\n",
    "                         'BehandlingsStatus',\n",
    "                         'ASAklass',\n",
    "                         'ForberedelsetidStartTidpunkt',\n",
    "                         'ForberedelsetidSlutTidpunkt',\n",
    "                         'PatientÅlderVidOp',\n",
    "                         'Veckodag',\n",
    "                         'Starttimme',\n",
    "                         'BMI',\n",
    "                         'Kroppslängd',\n",
    "                         'Kroppsvikt',\n",
    "                         'OperationstidStart',\n",
    "                         'AnestesitidStart'\n",
    "                        ]]\n",
    "behandling = behandling[behandling['BehandlingsStatus'] == 'Opererad'] # Remove 'abrutna' operationer as they do not contain all relevant data\n",
    "print(\"Behandling length: {}\".format(len(behandling)))\n",
    "\n",
    "# Load ingrepp and select relevant columns\n",
    "ingrepp = pd.read_csv(Path('../20210324/with_name/op_ingrepp_namn.csv'))\n",
    "ingrepp = ingrepp[['Der_Behandling_PK',\n",
    "                   'Ingreppkod',\n",
    "                   'Primär_Sekundär',\n",
    "                   'Sida',\n",
    "                  ]]\n",
    "ingrepp = ingrepp[ingrepp['Primär_Sekundär'] == 'Primär'] # Might want to include this if we make a more complicated model\n",
    "print(\"Ingrepp length: {}\".format(len(ingrepp)))\n",
    "\n",
    "# Load diagnos and select relevant columns\n",
    "diagnos = pd.read_csv(Path('../20210324/with_name/op_diagnos_namn.csv'))\n",
    "diagnos = diagnos[['Der_Behandling_PK',\n",
    "                   'Diagnoskod',\n",
    "                   'Primär_Sekundär',\n",
    "                  ]]\n",
    "diagnos = diagnos[diagnos['Primär_Sekundär'] == 'Primär'] # Might want to include this if we make a more complicated model\n",
    "print(\"Diagnos length: {}\".format(len(diagnos)))\n",
    "\n",
    "# Combine the data frames\n",
    "combined_df = behandling.merge(diagnos, on='Der_Behandling_PK').merge(ingrepp, on='Der_Behandling_PK')\n",
    "combined_df = combined_df.dropna()\n",
    "print(\"Combined length: {}\".format(len(combined_df)))\n",
    "\n",
    "ingreppsgrupp = []\n",
    "for index, row in combined_df.iterrows():\n",
    "    ingrepp = row['Ingreppkod']\n",
    "    ingrepp_group = ingrepp[0:2]\n",
    "    ingreppsgrupp.append(ingrepp_group)\n",
    "combined_df['IngreppsGrupp'] = ingreppsgrupp\n",
    "\n",
    "diagnosgrupp = []\n",
    "for index, row in combined_df.iterrows():\n",
    "    diagnos = row['Diagnoskod']\n",
    "    diagnos_grupp = diagnos[0]\n",
    "    diagnosgrupp.append(diagnos_grupp)\n",
    "combined_df['DiagnosGrupp'] = diagnosgrupp\n",
    "\n",
    "# Calculate and add time to the dataframe\n",
    "# Bad algoritm for checking min and max time of förbereds\n",
    "start_pre = combined_df[\"ForberedelsetidStartTidpunkt\"]\n",
    "slut_pre = combined_df[\"ForberedelsetidSlutTidpunkt\"]\n",
    "start_an = combined_df['AnestesitidStart']\n",
    "start_op = combined_df['OperationstidStart']\n",
    "\n",
    "start_times = []\n",
    "for time in start_pre:\n",
    "    minn = int(time[-9:-7])\n",
    "    hour = int(time[-12:-10])\n",
    "    minutes = hour*60 + minn\n",
    "    start_times.append(minutes)\n",
    "    \n",
    "stop_times = []\n",
    "for time in slut_pre:\n",
    "    minn = int(time[-9:-7])\n",
    "    hour = int(time[-12:-10])\n",
    "    minutes = hour*60 + minn\n",
    "    stop_times.append(minutes)\n",
    "\n",
    "an_times = []\n",
    "for time in start_an:\n",
    "    minn = int(time[-9:-7])\n",
    "    hour = int(time[-12:-10])\n",
    "    minutes = hour*60 + minn\n",
    "    an_times.append(minutes)\n",
    "    \n",
    "op_times = []\n",
    "for time in start_op:\n",
    "    minn = int(time[-9:-7])\n",
    "    hour = int(time[-12:-10])\n",
    "    minutes = hour*60 + minn\n",
    "    op_times.append(minutes)\n",
    "\n",
    "times = []\n",
    "for i in range(len(start_times)):\n",
    "    #print(stop_times[i], start_times[i], stop_times[i] - start_times[i])\n",
    "    times.append((stop_times[i] - start_times[i]) + (an_times[i] - op_times[i])) # Förberedelsetid + anestesiförberedelsetid\n",
    "    \n",
    "# Add total time to dataframe\n",
    "combined_df['Tid'] = times\n",
    "\n",
    "# Remove all fetuers we don't want\n",
    "features_df = combined_df.drop([\"Der_Behandling_PK\", \n",
    "                               \"Der_Opkort_FK\",\n",
    "                               \"Der_Anestesikort_FK\",\n",
    "                               \"BehandlingsStatus\",\n",
    "                               \"ForberedelsetidStartTidpunkt\",\n",
    "                               \"ForberedelsetidSlutTidpunkt\",\n",
    "                               \"Primär_Sekundär_x\",\n",
    "                               \"Primär_Sekundär_y\",\n",
    "                                \"AnestesitidStart\",\n",
    "                                \"OperationstidStart\"\n",
    "                            ], axis='columns')\n",
    "\n",
    "\n",
    "'''\n",
    "diagnosgrupper = {}\n",
    "for diagnosgrupp, diagnosgrupp_df in features_df.groupby('DiagnosGrupp'):\n",
    "    diagnosgrupper[diagnosgrupp] = diagnosgrupp_df\n",
    "grupp_mean = []\n",
    "grupp_std = []\n",
    "for grupp in diagnosgrupper.keys():\n",
    "    df = features_df[features_df['DiagnosGrupp'] == grupp]\n",
    "    grupp_mean.append(df['time'].mean())\n",
    "    grupp_std.append(df['time'].std())\n",
    "#plt.errorbar(diagnosgrupper.keys(), grupp_mean, grupp_std, marker='o', linestyle='None', capsize=3)\n",
    "\n",
    "ingreppsgrupper = {}\n",
    "for ingreppsgrupp, ingreppsgrupp_df in features_df.groupby('IngreppsGrupp'):\n",
    "    ingreppsgrupper[ingreppsgrupp] = ingreppsgrupp_df\n",
    "grupp_mean = []\n",
    "grupp_std = []\n",
    "for grupp in ingreppsgrupper.keys():\n",
    "    df = features_df[features_df['IngreppsGrupp'] == grupp]\n",
    "    grupp_mean.append(df['time'].mean())\n",
    "    grupp_std.append(df['time'].std())\n",
    "#plt.errorbar(ingreppsgrupper.keys(), grupp_mean, grupp_std, marker='o', linestyle='None', capsize=3)\n",
    "'''\n",
    "features_df = features_df.drop([\"Diagnoskod\", \"Ingreppkod\"], axis='columns')\n",
    "features_df = features_df[features_df['IngreppsGrupp'].isin(['NC', 'NH', 'NB', 'NG', 'NF', 'ND'])]\n",
    "\n",
    "# Instansiate Metrics so we can use MAAPE later\n",
    "metrics = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = features_df.dropna()\n",
    "y = features_df[\"Tid\"]\n",
    "X = features_df.drop(\"Tid\", axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = features_df['time']\n",
    "#X = features_df.drop('time', axis='columns')\n",
    "enc = OrdinalEncoder()\n",
    "X = enc.fit_transform(X)\n",
    "X = pd.DataFrame(X)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state=66)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.2, random_state=66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try dummyregressor as a basecase, anything worse than this is really terrible\n",
    "regr = DummyRegressor()\n",
    "regr.fit(X_train, y_train)\n",
    "pred = regr.predict(X_test)\n",
    "abs_error = mean_squared_error(y_test, pred, squared=False)\n",
    "percentage_error = metrics.mean_arctangent_absolute_percentage_error(clean=True, y_pred=np.array(pred), y_true=np.array(y_test))\n",
    "r2 = r2_score(y_test, pred)\n",
    "print(f'abs error:{abs_error} \\n% error: {percentage_error} \\nR-squared: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = DecisionTreeRegressor()\n",
    "regr.fit(X_train, y_train)\n",
    "pred = regr.predict(X_test)\n",
    "abs_error = mean_squared_error(y_test, pred, squared=False)\n",
    "percentage_error = metrics.mean_arctangent_absolute_percentage_error(clean=True, y_pred=np.array(pred), y_true=np.array(y_test))\n",
    "r2 = r2_score(y_test, pred)\n",
    "print(f'abs error:{abs_error} \\n% error: {percentage_error} \\nR-squared: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_regr = RandomForestRegressor(max_depth=22)\n",
    "forest_regr.fit(X_train, y_train)\n",
    "pred = forest_regr.predict(X_test)\n",
    "abs_error = mean_squared_error(y_test, pred, squared=False)\n",
    "percentage_error = metrics.mean_arctangent_absolute_percentage_error(clean=True, y_pred=np.array(pred), y_true=np.array(y_test))\n",
    "r2 = r2_score(y_test, pred)\n",
    "print(f'abs error:{abs_error} \\n% error: {percentage_error} \\nR-squared: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost_regr = GradientBoostingRegressor(max_depth=3,)\n",
    "boost_regr.fit(X_train, y_train)\n",
    "pred = boost_regr.predict(X_test)\n",
    "abs_error = mean_squared_error(y_test, pred, squared=False)\n",
    "percentage_error = metrics.mean_arctangent_absolute_percentage_error(clean=True, y_pred=np.array(pred), y_true=np.array(y_test))\n",
    "r2 = r2_score(y_test, pred)\n",
    "print(f'abs error:{abs_error} \\n% error: {percentage_error} \\nR-squared: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = MLPRegressor(random_state=1, activation='logistic', learning_rate='adaptive', )\n",
    "regr.fit(X_train, y_train)\n",
    "pred = regr.predict(X_test)\n",
    "abs_error = mean_squared_error(y_test, pred, squared=False)\n",
    "percentage_error = metrics.mean_arctangent_absolute_percentage_error(clean=True, y_pred=np.array(pred), y_true=np.array(y_test))\n",
    "r2 = r2_score(y_test, pred)\n",
    "print(f'abs error:{abs_error} \\n% error: {percentage_error} \\nR-squared: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "pred = regr.predict(X_test)\n",
    "abs_error = mean_squared_error(y_test, pred, squared=False)\n",
    "percentage_error = metrics.mean_arctangent_absolute_percentage_error(clean=True, y_pred=np.array(pred), y_true=np.array(y_test))\n",
    "r2 = r2_score(y_test, pred)\n",
    "print(f'abs error:{abs_error} \\n% error: {percentage_error} \\nR-squared: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
